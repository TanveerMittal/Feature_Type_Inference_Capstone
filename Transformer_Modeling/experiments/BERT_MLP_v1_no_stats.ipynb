{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b38105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from process_data import *\n",
    "from models.bert_mlp_v1_no_stats import *\n",
    "from training import train\n",
    "from evaluation import evaluate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56e6704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record_id</th>\n",
       "      <th>Attribute_name</th>\n",
       "      <th>y_act</th>\n",
       "      <th>total_vals</th>\n",
       "      <th>num_nans</th>\n",
       "      <th>%_nans</th>\n",
       "      <th>num_of_dist_val</th>\n",
       "      <th>%_dist_val</th>\n",
       "      <th>mean</th>\n",
       "      <th>std_dev</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_stopword_total</th>\n",
       "      <th>stdev_stopword_total</th>\n",
       "      <th>mean_char_count</th>\n",
       "      <th>stdev_char_count</th>\n",
       "      <th>mean_whitespace_count</th>\n",
       "      <th>stdev_whitespace_count</th>\n",
       "      <th>mean_delim_count</th>\n",
       "      <th>stdev_delim_count</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_long_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Area</td>\n",
       "      <td>categorical</td>\n",
       "      <td>21477</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174</td>\n",
       "      <td>0.810169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.816638</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Area Code</td>\n",
       "      <td>categorical</td>\n",
       "      <td>21477</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174</td>\n",
       "      <td>0.810169</td>\n",
       "      <td>125.449411</td>\n",
       "      <td>72.866452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>Element</td>\n",
       "      <td>categorical</td>\n",
       "      <td>21477</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>Element Code</td>\n",
       "      <td>categorical</td>\n",
       "      <td>21477</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>5211.687154</td>\n",
       "      <td>146.816661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Item</td>\n",
       "      <td>categorical</td>\n",
       "      <td>21477</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115</td>\n",
       "      <td>0.535457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.244994</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Record_id Attribute_name        y_act  total_vals  num_nans  %_nans  \\\n",
       "0         33           Area  categorical       21477         0     0.0   \n",
       "1         33      Area Code  categorical       21477         0     0.0   \n",
       "2         33        Element  categorical       21477         0     0.0   \n",
       "3         33   Element Code  categorical       21477         0     0.0   \n",
       "4         33           Item  categorical       21477         0     0.0   \n",
       "\n",
       "   num_of_dist_val  %_dist_val         mean     std_dev  ...  \\\n",
       "0              174    0.810169     0.000000    0.000000  ...   \n",
       "1              174    0.810169   125.449411   72.866452  ...   \n",
       "2                2    0.009312     0.000000    0.000000  ...   \n",
       "3                2    0.009312  5211.687154  146.816661  ...   \n",
       "4              115    0.535457     0.000000    0.000000  ...   \n",
       "\n",
       "   mean_stopword_total  stdev_stopword_total mean_char_count stdev_char_count  \\\n",
       "0                  0.2                   0.4            10.0         4.816638   \n",
       "1                  0.0                   0.0             1.0         0.000000   \n",
       "2                  0.0                   0.0             4.0         0.000000   \n",
       "3                  0.0                   0.0             4.0         0.000000   \n",
       "4                  0.8                   0.4            19.6         2.244994   \n",
       "\n",
       "  mean_whitespace_count stdev_whitespace_count mean_delim_count  \\\n",
       "0                   0.4                    0.8              0.4   \n",
       "1                   0.0                    0.0              0.0   \n",
       "2                   0.0                    0.0              0.0   \n",
       "3                   0.0                    0.0              0.0   \n",
       "4                   2.0                    0.0              2.0   \n",
       "\n",
       "   stdev_delim_count  is_list  is_long_sentence  \n",
       "0                0.8    False             False  \n",
       "1                0.0    False             False  \n",
       "2                0.0    False             False  \n",
       "3                0.0    False             False  \n",
       "4                0.0    False             False  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zoo_train, df_zoo_test = load_data(\"../data\")\n",
    "df_zoo_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9965ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Area [SEP] Afghanistan [SEP] Albania [SEP] Alg...</td>\n",
       "      <td>[21477.0, 0.0, 0.0, 174.0, 0.810169018, 0.0, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Area Code [SEP] 2 [SEP] 3 [SEP] 4 [SEP] 7 [SEP] 8</td>\n",
       "      <td>[21477.0, 0.0, 0.0, 174.0, 0.810169018, 125.44...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Element [SEP] Food [SEP] Feed [SEP] Food [SEP]...</td>\n",
       "      <td>[21477.0, 0.0, 0.0, 2.0, 0.009312288, 0.0, 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Element Code [SEP] 5142 [SEP] 5521 [SEP] 5142 ...</td>\n",
       "      <td>[21477.0, 0.0, 0.0, 2.0, 0.009312288, 5211.687...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Item [SEP] Wheat and products [SEP] Rice (Mill...</td>\n",
       "      <td>[21477.0, 0.0, 0.0, 115.0, 0.535456535, 0.0, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Area [SEP] Afghanistan [SEP] Albania [SEP] Alg...   \n",
       "1  Area Code [SEP] 2 [SEP] 3 [SEP] 4 [SEP] 7 [SEP] 8   \n",
       "2  Element [SEP] Food [SEP] Feed [SEP] Food [SEP]...   \n",
       "3  Element Code [SEP] 5142 [SEP] 5521 [SEP] 5142 ...   \n",
       "4  Item [SEP] Wheat and products [SEP] Rice (Mill...   \n",
       "\n",
       "                                            features  label  \n",
       "0  [21477.0, 0.0, 0.0, 174.0, 0.810169018, 0.0, 0...      1  \n",
       "1  [21477.0, 0.0, 0.0, 174.0, 0.810169018, 125.44...      1  \n",
       "2  [21477.0, 0.0, 0.0, 2.0, 0.009312288, 0.0, 0.0...      1  \n",
       "3  [21477.0, 0.0, 0.0, 2.0, 0.009312288, 5211.687...      1  \n",
       "4  [21477.0, 0.0, 0.0, 115.0, 0.535456535, 0.0, 0...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = preprocess_bert(df_zoo_train)\n",
    "test_data = preprocess_bert(df_zoo_test)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60dcc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_data[['text', \"features\"]], train_data['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc1a639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b266e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f834770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb468066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tamittal/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2198: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = init_dataloaders_bert(x_train, y_train, x_val, y_val, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876f07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_mlp_v1_no_stats(bert)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "543beca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e7df9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.30349971 0.4756125  1.64413364 3.04022989 7.66666667 1.88088889\n",
      " 4.64035088 1.01050621 1.25281231]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tamittal/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass classes=[0 1 2 3 4 5 6 7 8], y=2190    0\n",
      "2921    0\n",
      "4042    7\n",
      "3603    0\n",
      "6365    0\n",
      "       ..\n",
      "4799    8\n",
      "1221    7\n",
      "7337    5\n",
      "4938    1\n",
      "5419    7\n",
      "Name: label, Length: 6348, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2593402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e30a31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36601513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.977\n",
      "Validation Loss: 1.901\n",
      "Validation Accuracy: 0.402\n",
      "\n",
      " Epoch 2 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.882\n",
      "Validation Loss: 1.812\n",
      "Validation Accuracy: 0.499\n",
      "\n",
      " Epoch 3 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.818\n",
      "Validation Loss: 1.746\n",
      "Validation Accuracy: 0.498\n",
      "\n",
      " Epoch 4 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.751\n",
      "Validation Loss: 1.688\n",
      "Validation Accuracy: 0.486\n",
      "\n",
      " Epoch 5 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.705\n",
      "Validation Loss: 1.638\n",
      "Validation Accuracy: 0.508\n",
      "\n",
      " Epoch 6 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.664\n",
      "Validation Loss: 1.593\n",
      "Validation Accuracy: 0.531\n",
      "\n",
      " Epoch 7 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.621\n",
      "Validation Loss: 1.551\n",
      "Validation Accuracy: 0.514\n",
      "\n",
      " Epoch 8 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.591\n",
      "Validation Loss: 1.524\n",
      "Validation Accuracy: 0.539\n",
      "\n",
      " Epoch 9 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.552\n",
      "Validation Loss: 1.484\n",
      "Validation Accuracy: 0.531\n",
      "\n",
      " Epoch 10 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.514\n",
      "Validation Loss: 1.444\n",
      "Validation Accuracy: 0.566\n",
      "\n",
      " Epoch 11 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.490\n",
      "Validation Loss: 1.415\n",
      "Validation Accuracy: 0.563\n",
      "\n",
      " Epoch 12 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.468\n",
      "Validation Loss: 1.389\n",
      "Validation Accuracy: 0.564\n",
      "\n",
      " Epoch 13 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.433\n",
      "Validation Loss: 1.358\n",
      "Validation Accuracy: 0.566\n",
      "\n",
      " Epoch 14 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.393\n",
      "Validation Loss: 1.332\n",
      "Validation Accuracy: 0.584\n",
      "\n",
      " Epoch 15 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.394\n",
      "Validation Loss: 1.308\n",
      "Validation Accuracy: 0.558\n",
      "\n",
      " Epoch 16 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.360\n",
      "Validation Loss: 1.279\n",
      "Validation Accuracy: 0.590\n",
      "\n",
      " Epoch 17 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.330\n",
      "Validation Loss: 1.259\n",
      "Validation Accuracy: 0.589\n",
      "\n",
      " Epoch 18 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.312\n",
      "Validation Loss: 1.236\n",
      "Validation Accuracy: 0.598\n",
      "\n",
      " Epoch 19 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.299\n",
      "Validation Loss: 1.215\n",
      "Validation Accuracy: 0.595\n",
      "\n",
      " Epoch 20 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.275\n",
      "Validation Loss: 1.188\n",
      "Validation Accuracy: 0.605\n",
      "\n",
      " Epoch 21 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.253\n",
      "Validation Loss: 1.168\n",
      "Validation Accuracy: 0.623\n",
      "\n",
      " Epoch 22 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.239\n",
      "Validation Loss: 1.156\n",
      "Validation Accuracy: 0.612\n",
      "\n",
      " Epoch 23 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.225\n",
      "Validation Loss: 1.138\n",
      "Validation Accuracy: 0.611\n",
      "\n",
      " Epoch 24 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.192\n",
      "Validation Loss: 1.113\n",
      "Validation Accuracy: 0.612\n",
      "\n",
      " Epoch 25 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.190\n",
      "Validation Loss: 1.101\n",
      "Validation Accuracy: 0.623\n",
      "\n",
      " Epoch 26 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.174\n",
      "Validation Loss: 1.088\n",
      "Validation Accuracy: 0.616\n",
      "\n",
      " Epoch 27 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.157\n",
      "Validation Loss: 1.063\n",
      "Validation Accuracy: 0.630\n",
      "\n",
      " Epoch 28 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.140\n",
      "Validation Loss: 1.050\n",
      "Validation Accuracy: 0.645\n",
      "\n",
      " Epoch 29 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.107\n",
      "Validation Loss: 1.032\n",
      "Validation Accuracy: 0.632\n",
      "\n",
      " Epoch 30 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.115\n",
      "Validation Loss: 1.022\n",
      "Validation Accuracy: 0.628\n",
      "\n",
      " Epoch 31 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.090\n",
      "Validation Loss: 1.008\n",
      "Validation Accuracy: 0.636\n",
      "\n",
      " Epoch 32 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.071\n",
      "Validation Loss: 0.994\n",
      "Validation Accuracy: 0.642\n",
      "\n",
      " Epoch 33 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.061\n",
      "Validation Loss: 0.983\n",
      "Validation Accuracy: 0.641\n",
      "\n",
      " Epoch 34 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.049\n",
      "Validation Loss: 0.970\n",
      "Validation Accuracy: 0.643\n",
      "\n",
      " Epoch 35 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.035\n",
      "Validation Loss: 0.961\n",
      "Validation Accuracy: 0.636\n",
      "\n",
      " Epoch 36 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.019\n",
      "Validation Loss: 0.944\n",
      "Validation Accuracy: 0.645\n",
      "\n",
      " Epoch 37 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.008\n",
      "Validation Loss: 0.933\n",
      "Validation Accuracy: 0.645\n",
      "\n",
      " Epoch 38 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.008\n",
      "Validation Loss: 0.924\n",
      "Validation Accuracy: 0.654\n",
      "\n",
      " Epoch 39 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.000\n",
      "Validation Loss: 0.913\n",
      "Validation Accuracy: 0.647\n",
      "\n",
      " Epoch 40 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.984\n",
      "Validation Loss: 0.901\n",
      "Validation Accuracy: 0.657\n",
      "\n",
      " Epoch 41 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.974\n",
      "Validation Loss: 0.896\n",
      "Validation Accuracy: 0.654\n",
      "\n",
      " Epoch 42 / 50\n",
      "  Batch    50  of    199.\n",
      "  Batch   100  of    199.\n",
      "  Batch   150  of    199.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.969\n",
      "Validation Loss: 0.893\n",
      "Validation Accuracy: 0.661\n",
      "\n",
      " Epoch 43 / 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1009/326739440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSC_180A/Feature_Type_Inference_Capstone/Transformer_Modeling/experiments/../training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, optimizer, cross_entropy)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# add on to the total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# backward pass to calculate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _, = train(model, train_dataloader, optimizer, cross_entropy)\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _, valid_acc = evaluate(model, val_dataloader, cross_entropy, y_val)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "    print(f'Validation Accuracy: {valid_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.plot(valid_losses)\n",
    "plt.legend([\"train\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36fee864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     63.\n",
      "test_accuracy: 0.39143576826196474\n"
     ]
    }
   ],
   "source": [
    "loss, preds, acc = evaluate(model,test_dataloader, cross_entropy, test_data[\"label\"])\n",
    "print(\"test_accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a04cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
